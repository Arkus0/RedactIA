import { GoogleGenAI, GenerateContentResponse, Type } from "@google/genai";
import { RedactionOptions, Length, Source, Tone, Thesis, StructureType, ModelId } from '../types';

const getClient = () => {
  return new GoogleGenAI({ apiKey: process.env.API_KEY });
};

const getModelConfig = (length: Length) => {
  switch (length) {
    case Length.SHORT:
      return { thinkingBudget: 1024, maxOutputTokens: 8192 };
    case Length.MEDIUM:
      return { thinkingBudget: 2048, maxOutputTokens: 16384 };
    // Para modo modular, los tokens son por secci√≥n, as√≠ que usamos configuraci√≥n est√°ndar
    default:
      return { thinkingBudget: 4096, maxOutputTokens: 32768 };
  }
};

const buildContextBlock = (sources: Source[]) => 
    sources.map(s => `<document name="${s.name}">${s.content.slice(0, 20000)}</document>`).join('\n'); // Limitamos contexto por seguridad

const cleanJsonOutput = (text: string): string => {
  if (!text) return "[]";
  return text.replace(/```json/g, '').replace(/```/g, '').trim();
};

// --- PROMPT PORTABLE (MEJORADO: XML SYSTEM PROMPT) ---
export const buildPortablePrompt = (
  sources: Source[], 
  instruction: string, 
  options: RedactionOptions
): string => {
  
  // Lista de archivos para que el usuario sepa qu√© adjuntar
  const fileManifest = sources.length > 0 
    ? sources.map(s => `    <file_requirement status="USER_MUST_ATTACH">${s.name}</file_requirement>`).join('\n')
    : "    <file_requirement>El usuario adjuntar√° los documentos relevantes.</file_requirement>";

  // Bloque de Estilo Avanzado
  const styleBlock = options.styleGuide 
    ? `  <style_dna_imperative>
    <description>Se ha proporcionado un ADN de estilo espec√≠fico. La IA DEBE mimetizar esta voz, vocabulario y cadencia rigurosamente.</description>
    <voice_sample>
${options.styleGuide}
    </voice_sample>
  </style_dna_imperative>`
    : `  <style_configuration>
    <tone>${options.tone}</tone>
    <register>Acad√©mico / Formal / Experto</register>
  </style_configuration>`;

  // Estructura XML optimizada para Claude/Grok/GPT-4o
  return `<!-- SYSTEM PROMPT: REDACTA_IA UNIVERSITY EXPORT -->
<!-- INSTRUCCI√ìN PARA EL USUARIO: Copia este prompt y ADJUNTA los archivos originales (PDF) en el chat con tu IA favorita. -->

<system_role>
  Act√∫a como un Profesor Universitario Titular y Editor Acad√©mico de clase mundial.
  Tu objetivo es producir textos de alta densidad intelectual, rigurosidad metodol√≥gica y fluidez narrativa, bas√°ndote EXCLUSIVAMENTE en la evidencia proporcionada.
</system_role>

<context_input>
  <instruction>
    El usuario proporcionar√° archivos adjuntos al chat. Tu an√°lisis debe basarse en esos documentos.
  </instruction>
  <manifest_of_expected_files>
${fileManifest}
  </manifest_of_expected_files>
</context_input>

<project_configuration>
  <output_format>${options.format}</output_format>
  <target_length>${options.length}</target_length>
  <structure_archetype>${options.structure}</structure_archetype>
  <citation_style>${options.includeCrossReferences ? 'APA 7 (Citas expl√≠citas requeridas: Autor, A√±o)' : 'Narrativa / Referencial'}</citation_style>
</project_configuration>

${styleBlock}

<task_directive>
  ${instruction}
</task_directive>

<execution_protocol>
  <phase_1_analysis>
    Analiza profundamente los documentos adjuntos. Extrae tesis centrales, contra-argumentos, datos duros y matices sutiles.
  </phase_1_analysis>
  <phase_2_synthesis>
    Integra la informaci√≥n. No hagas res√∫menes lineales; sintetiza por temas o argumentos (Sintop√≠a).
  </phase_2_synthesis>
  <phase_3_production>
    Redacta el contenido usando Markdown jer√°rquico y limpio.
    - Usa H1 para el T√≠tulo Principal.
    - Usa H2 y H3 para organizar secciones y subsecciones l√≥gicas.
    - Emplea negritas para resaltar conceptos clave (sin abusar).
  </phase_3_production>
  <phase_4_quality_check>
    Asegura que el tono sea consistente y que NO existan alucinaciones fuera de los documentos proporcionados.
  </phase_4_quality_check>
</execution_protocol>

<output_trigger>
  Si has recibido los archivos adjuntos, procede a generar la respuesta siguiendo la <task_directive>.
</output_trigger>`.trim();
};

// --- 1. GENERADOR DE TESIS (El Estratega) ---
export const generateTheses = async (sources: Source[], instruction: string): Promise<Thesis[]> => {
  const ai = getClient();
  const context = buildContextBlock(sources);
  const modelId = 'gemini-3-flash-preview';

  const prompt = `
    Act√∫a como un Tutor de Universidad Senior.
    Analiza las siguientes fuentes y la instrucci√≥n del alumno.
    <library>${context || "Conocimiento General"}</library>
    <instruction>${instruction || "Analizar el tema principal"}</instruction>
    
    TU TAREA: Prop√≥n 3 enfoques (Tesis) acad√©micos s√≥lidos.
    SALIDA: JSON Array v√°lido. [{ "id": "1", "angle": "Cr√≠tico", "title": "...", "description": "..." }]
  `;

  try {
    const resp = await ai.models.generateContent({
      model: modelId,
      contents: prompt,
      config: { responseMimeType: "application/json" }
    });
    return JSON.parse(cleanJsonOutput(resp.text || "[]")) as Thesis[];
  } catch (e) {
    console.error("Error tesis:", e);
    return [];
  }
};

// --- 2. GENERADOR DE √çNDICE (El Arquitecto - Solo para textos largos) ---
const generateOutline = async (sources: Source[], instruction: string, thesis: Thesis | null, options: RedactionOptions): Promise<string[]> => {
  const ai = getClient();
  const context = buildContextBlock(sources);
  const sectionsCount = options.length === Length.THESIS ? "8-12" : "5-7";
  
  const prompt = `
    Act√∫a como Arquitecto de Contenidos Acad√©micos.
    TAREA: Crea un √çNDICE ESTRUCTURADO (Outline) para un trabajo de longitud: ${options.length}.
    Debe tener aprox ${sectionsCount} secciones principales.
    
    TEMA: ${instruction}
    TESIS CENTRAL: ${thesis ? thesis.title : "N/A"}
    ESTRUCTURA: ${options.structure}
    CONTEXTO: <library>${context.slice(0, 10000)}</library>

    SALIDA: Solo devuelve un JSON Array de strings con los t√≠tulos.
    Ej: ["Introducci√≥n", "Cap√≠tulo 1: Historia", "Cap√≠tulo 2: An√°lisis", "Conclusi√≥n"]
  `;

  try {
    const resp = await ai.models.generateContent({
      model: 'gemini-3-flash-preview', // Flash es excelente para estructuras
      contents: prompt,
      config: { responseMimeType: "application/json" }
    });
    return JSON.parse(cleanJsonOutput(resp.text || "[]"));
  } catch (e) {
    console.error("Error outline:", e);
    return ["Introducci√≥n", "Desarrollo", "Conclusi√≥n"];
  }
};

// --- 3. PIPELINE PRINCIPAL DE REDACCI√ìN ---
type StreamCallback = (chunk: string) => void;
type ResetCallback = () => void;

export const generateRedaction = async (
  sources: Source[],
  instruction: string, 
  options: RedactionOptions,
  selectedThesis: Thesis | null,
  onChunk: StreamCallback,
  onReset: ResetCallback
): Promise<string> => {
  const ai = getClient();
  const selectedModel = options.model || ModelId.GEMINI_3_PRO;
  
  // Decisi√≥n de Estrategia
  const isModular = options.length === Length.LONG || options.length === Length.THESIS;

  if (isModular) {
    return generateModularRedaction(ai, selectedModel, sources, instruction, options, selectedThesis, onChunk, onReset);
  } else {
    return generateSinglePassRedaction(ai, selectedModel, sources, instruction, options, selectedThesis, onChunk, onReset);
  }
};

// --- ESTRATEGIA A: SINGLE PASS (Textos Cortos/Medios) ---
const generateSinglePassRedaction = async (
    ai: GoogleGenAI, 
    model: string,
    sources: Source[],
    instruction: string, 
    options: RedactionOptions,
    selectedThesis: Thesis | null,
    onChunk: StreamCallback,
    onReset: ResetCallback
): Promise<string> => {
    
    const context = buildContextBlock(sources);
    const { thinkingBudget, maxOutputTokens } = getModelConfig(options.length);

    // Prompt unificado
    const prompt = `
      Act√∫a como Estudiante de Doctorado.
      OBJETIVO: Escribir un trabajo acad√©mico impecable.
      
      Configuraci√≥n:
      - Tono: ${options.tone}
      - Formato: ${options.format}
      - Tesis: ${selectedThesis?.title || "N/A"}
      ${options.styleGuide ? `‚ö†Ô∏è ESTILO OBLIGATORIO: ${options.styleGuide}` : ""}
      
      Instrucci√≥n: ${instruction}
      Fuentes: ${context}
    `;

    // Configuraci√≥n Thinking
    const config: any = { maxOutputTokens, temperature: 0.7 };
    if (model.includes('gemini-3') || model.includes('gemini-2.5')) {
       config.thinkingConfig = { thinkingBudget };
    }

    let fullText = '';
    
    // Paso 1: Redacci√≥n
    const draftResp = await ai.models.generateContentStream({
      model: model,
      contents: prompt,
      config: config
    });

    for await (const chunk of draftResp) {
      const t = (chunk as GenerateContentResponse).text || '';
      fullText += t;
      if (!options.criticMode) onChunk(t);
    }

    // Paso 2 (Opcional): Cr√≠tico / Tutor
    if (options.criticMode) {
        onReset();
        onChunk(fullText); // Restaurar texto original visualmente
        onChunk("\n\n_üë®‚Äçüè´ El Tutor Virtual est√° revisando el texto..._\n\n");
        
        const critiquePrompt = `
           Act√∫a como Profesor. Revisa este texto.
           Si es bueno, no cambies nada. Si hay errores l√≥gicos o de tono, corr√≠gelos.
           A√±ade al final: "--- üìù INFORME DEL TUTOR ---" con 3 puntos clave.
           Texto: ${fullText}
        `;
        
        const critResp = await ai.models.generateContentStream({
            model: model, 
            contents: critiquePrompt
        });
        
        let improved = "";
        onReset();
        for await (const chunk of critResp) {
            const t = (chunk as GenerateContentResponse).text || '';
            improved += t;
            onChunk(t);
        }
        fullText = improved;
    }

    return fullText;
};

// --- ESTRATEGIA B: MODULAR (Textos Largos - Chain of Density) ---
const generateModularRedaction = async (
    ai: GoogleGenAI, 
    model: string,
    sources: Source[],
    instruction: string, 
    options: RedactionOptions,
    selectedThesis: Thesis | null,
    onChunk: StreamCallback,
    onReset: ResetCallback
): Promise<string> => {
    const context = buildContextBlock(sources);

    // 1. Arquitecto
    onChunk(`_üèóÔ∏è El Arquitecto est√° dise√±ando la estructura para un trabajo ${options.length}..._\n\n`);
    const outline = await generateOutline(sources, instruction, selectedThesis, options);
    onReset();

    let fullDocument = "";
    let previousContext = "Inicio del documento.";

    // 2. Constructor (Loop)
    for (let i = 0; i < outline.length; i++) {
        const sectionTitle = outline[i];
        
        onChunk(`\n\n## ${sectionTitle} \n\n`); // Renderizar t√≠tulo
        fullDocument += `## ${sectionTitle}\n\n`;

        const sectionPrompt = `
            ESTAMOS ESCRIBIENDO UN TRABAJO EXTENSO Y COHERENTE.
            
            ESTRUCTURA TOTAL: ${JSON.stringify(outline)}
            SECCI√ìN AHORA: "${sectionTitle}"
            CONTEXTO PREVIO (Resumen): "${previousContext.slice(-3000)}"
            
            INSTRUCCIONES:
            1. Desarrolla ESTA secci√≥n en profundidad.
            2. Usa Markdown.
            3. ${options.includeCrossReferences ? "Cita fuentes (Autor, A√±o)." : ""}
            4. Tono: ${options.tone}.
            ${options.styleGuide ? `5. ‚ö†Ô∏è ESTILO: ${options.styleGuide}` : ""}
            
            FUENTES DISPONIBLES:
            ${context}
        `;

        // Usamos el modelo seleccionado. Si es 'gemini-3-pro', usaremos thinking moderado por secci√≥n.
        const config: any = { maxOutputTokens: 8192 }; // Tokens suficientes por secci√≥n
        if (model.includes('gemini-3') || model.includes('gemini-2.5')) {
             config.thinkingConfig = { thinkingBudget: 2048 }; // Thinking moderado por cap√≠tulo
        }

        let sectionText = "";
        try {
            const resp = await ai.models.generateContentStream({
                model: model,
                contents: sectionPrompt,
                config: config
            });

            for await (const chunk of resp) {
                const t = (chunk as GenerateContentResponse).text || '';
                sectionText += t;
                onChunk(t);
            }
        } catch (e) {
            onChunk(`\n_[Error generando secci√≥n ${sectionTitle}. Continuando...]_`);
        }

        fullDocument += sectionText + "\n\n";
        previousContext += `\nResumen de ${sectionTitle}: ` + sectionText.slice(0, 500) + "..."; // Acumulamos contexto
    }

    // 3. Cierre (Si hay modo cr√≠tico, a√±adimos un informe final, no reescribimos todo para no gastar infinito)
    if (options.criticMode) {
        onChunk("\n\n_üë®‚Äçüè´ Generando Informe Final del Tutor..._\n\n");
        const reportPrompt = `Genera un breve informe acad√©mico evaluando la estructura y contenido de este trabajo. T√≠tulo: INFORME FINAL.`;
        const report = await ai.models.generateContent({ model: 'gemini-3-flash-preview', contents: reportPrompt });
        const text = report.text || "";
        onChunk(text);
        fullDocument += "\n\n" + text;
    }

    return fullDocument;
};

// --- UTILS ADICIONALES ---
export const generateOptimizedPrompt = async (sources: Source[], instruction: string): Promise<string> => {
  const ai = getClient();
  const prompt = `Mejora esta instrucci√≥n acad√©mica: "${instruction}". Hazla detallada y estructurada.`;
  const resp = await ai.models.generateContent({ model: 'gemini-3-flash-preview', contents: prompt });
  return resp.text?.trim() || instruction;
};

export const generateStyleGuide = async (samples: string[]): Promise<string> => {
  const ai = getClient();
  const prompt = `Analiza estilo: ${samples.join('\n')}. Crea prompt de sistema para imitarlo.`;
  const resp = await ai.models.generateContent({ model: 'gemini-3-pro-preview', contents: prompt });
  return resp.text || "";
};