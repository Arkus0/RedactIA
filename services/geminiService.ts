import { GoogleGenAI, GenerateContentResponse } from "@google/genai";
import { RedactionOptions, Length, Source, Thesis, ModelId } from '../types';

const getClient = () => {
  return new GoogleGenAI({ apiKey: process.env.API_KEY });
};

// Convierte nuestras fuentes a formato nativo de Gemini (Text Parts + Inline Data Parts)
const buildContentParts = (sources: Source[], additionalPrompt: string) => {
  const parts: any[] = [];

  // 1. A√±adimos los PDFs como im√°genes/datos binarios (Multimodalidad Real)
  sources.forEach(source => {
    if (source.mimeType === 'application/pdf') {
      parts.push({
        inlineData: {
          mimeType: 'application/pdf',
          data: source.content // Base64 puro
        }
      });
    } else {
      parts.push({
        text: `FUENTE DE TEXTO [${source.name}]:\n${source.content}\n---`
      });
    }
  });

  // 2. A√±adimos el prompt de texto al final
  parts.push({ text: additionalPrompt });

  return parts;
};

const cleanJsonOutput = (text: string): string => {
  if (!text) return "[]";
  return text.replace(/```json/g, '').replace(/```/g, '').trim();
};

// --- 1. GENERADOR DE TESIS (ESTRATEGA) ---
export const generateTheses = async (sources: Source[], instruction: string): Promise<Thesis[]> => {
  const ai = getClient();
  
  const prompt = `
    ACT√öA COMO: Profesor Universitario Senior.
    TAREA: Analizar los documentos adjuntos y la instrucci√≥n del usuario para proponer 3 enfoques (Tesis) s√≥lidos.
    INSTRUCCI√ìN USUARIO: "${instruction}"
    
    SALIDA ESPERADA: JSON Array.
    [{ "id": "1", "angle": "Cr√≠tico", "title": "T√≠tulo Acad√©mico Sugerido", "description": "Breve explicaci√≥n del enfoque..." }]
  `;

  try {
    const resp = await ai.models.generateContent({
      model: 'gemini-2.0-flash', 
      contents: {
        parts: buildContentParts(sources, prompt)
      },
      config: { responseMimeType: "application/json" }
    });
    return JSON.parse(cleanJsonOutput(resp.text || "[]"));
  } catch (e) {
    console.error("Error tesis:", e);
    return [];
  }
};

// --- 2. ARQUITECTO DE ESTRUCTURA ---
const generateOutline = async (sources: Source[], instruction: string, thesis: Thesis | null, options: RedactionOptions): Promise<string[]> => {
  const ai = getClient();
  const sectionsCount = options.length === Length.THESIS ? "10-15" : "6-8";
  
  const prompt = `
    ACT√öA COMO: Editor Acad√©mico.
    OBJETIVO: Crear un √çndice Estructural (Outline) coherente para un trabajo de longitud: ${options.length}.
    TESIS CENTRAL: ${thesis?.title || "An√°lisis General"} (${thesis?.description})
    INSTRUCCI√ìN: ${instruction}
    
    Genera una lista de ${sectionsCount} t√≠tulos de secciones l√≥gicas.
    SALIDA: JSON Array de strings.
  `;

  try {
    const resp = await ai.models.generateContent({
      model: 'gemini-2.0-flash',
      contents: { parts: buildContentParts(sources, prompt) },
      config: { responseMimeType: "application/json" }
    });
    return JSON.parse(cleanJsonOutput(resp.text || "[]"));
  } catch (e) {
    return ["Introducci√≥n", "Desarrollo Te√≥rico", "An√°lisis de Evidencia", "Discusi√≥n", "Conclusi√≥n"];
  }
};

// --- HUMANIZADOR (INTERNAL) ---
const humanizeContentLogic = async (ai: any, text: string, options: RedactionOptions): Promise<string> => {
    const prompt = `
        TU √öNICA MISI√ìN: POST-PRODUCCI√ìN "ANTI-IA".
        
        INPUT: Un texto generado por una IA (probablemente Claude o GPT) que suena "demasiado perfecto" y rob√≥tico.
        
        OUTPUT: El mismo contenido, reescrito por un humano real.

        REGLAS DE DESTRUCCI√ìN DE PATRONES:
        1. üî• **MUERTE A LAS LISTAS**: Si ves bullet points, CONVI√âRTELOS en p√°rrafos densos y narrativos. (Esencial).
        2. üö´ **VOCABULARIO PROHIBIDO**: Elimina: "Cabe destacar", "En conclusi√≥n", "Es fundamental", "Ahondar", "Panorama", "Tapiz".
        3. üåä **CAOS ESTRUCTURAL (Burstiness)**: Alterna frases muy largas (subordinadas) con frases de 3 palabras. Rompe el ritmo hipn√≥tico de la IA.
        4. üß¨ **ADN PERSONAL**: ${options.styleGuide ? 'Imita estrictamente este estilo: ' + options.styleGuide : 'Usa un tono ' + options.tone + ' pero imperfecto/natural.'}
        
        IMPORTANTE:
        - No cambies datos f√°cticos (fechas, nombres).
        - No seas "educado". S√© directo.
        - Introduce conectores naturales: "Y es que...", "Total,", "Al final del d√≠a...", "Lo curioso es que...".
        
        TEXTO A HUMANIZAR:
        "${text}"
    `;

    // Usamos temperatura alta (1.1) para romper la predictibilidad estad√≠stica
    const resp = await ai.models.generateContent({
        model: 'gemini-2.0-flash',
        contents: prompt,
        config: { 
            temperature: 1.1, 
            topP: 0.95, 
        }
    });

    return resp.text || text;
};

// --- 3. EXPOSED: HUMANIZADOR DIRECTO (PARA TEXTO PEGADO) ---
export const humanizeTextOnly = async (
    text: string, 
    options: RedactionOptions,
    onChunk: (c: string) => void
): Promise<string> => {
    const ai = getClient();
    
    // Si el texto es muy largo, lo partimos? Gemini 2.0 aguanta mucho, enviamos de golpe.
    // Simular streaming para UX
    onChunk("Detectando patrones de IA en el texto de entrada...\n");
    await new Promise(r => setTimeout(r, 600));
    onChunk("Aplicando imperfecciones y estilo personal...\n\n");
    
    const result = await humanizeContentLogic(ai, text, options);
    
    // Limpiar el loading text simulado antes de devolver
    return result; 
};

// --- 4. MOTOR DE REDACCI√ìN (PDF -> TEXTO) ---
export const generateRedaction = async (
  sources: Source[],
  instruction: string, 
  options: RedactionOptions,
  selectedThesis: Thesis | null,
  onChunk: (c: string) => void,
  onReset: () => void
): Promise<string> => {
  const ai = getClient();
  const isModular = options.length === Length.LONG || options.length === Length.THESIS;
  const logicModel = 'gemini-2.0-flash'; 

  if (isModular) {
    return generateModularRedaction(ai, logicModel, sources, instruction, options, selectedThesis, onChunk, onReset);
  } else {
    return generateSinglePassRedaction(ai, logicModel, sources, instruction, options, selectedThesis, onChunk, onReset);
  }
};

const generateSinglePassRedaction = async (ai: any, model: string, sources: Source[], instruction: string, options: RedactionOptions, thesis: any, onChunk: any, onReset: any) => {
    const prompt = `
        Escribe un trabajo acad√©mico completo.
        TEMA: ${instruction}
        ENFOQUE: ${thesis?.title || "Est√°ndar"}
        LONGITUD: ${options.length}
        
        INSTRUCCIONES CLAVE:
        - Basa tus argumentos EXCLUSIVAMENTE en los documentos adjuntos.
        - Estructura: T√≠tulo, Introducci√≥n, Cuerpo, Conclusi√≥n.
    `;

    let draft = "";
    const draftResp = await ai.models.generateContentStream({
        model: model,
        contents: { parts: buildContentParts(sources, prompt) },
        config: { maxOutputTokens: 8192 } 
    });

    for await (const chunk of draftResp) {
        const t = (chunk as GenerateContentResponse).text || '';
        draft += t;
        onChunk(t);
    }

    if (options.length !== Length.SHORT) {
        onReset();
        onChunk(draft);
        onChunk("\n\n_üß¨ Humanizando y verificando coherencia..._");
        const humanized = await humanizeContentLogic(ai, draft, options);
        onReset();
        onChunk(humanized);
        return humanized;
    }
    return draft;
};

const generateModularRedaction = async (ai: any, model: string, sources: Source[], instruction: string, options: RedactionOptions, thesis: any, onChunk: any, onReset: any) => {
    onChunk(`_üèóÔ∏è Analizando ${sources.length} documentos y dise√±ando estructura..._\n\n`);
    
    const outline = await generateOutline(sources, instruction, thesis, options);
    onReset();

    let fullDocument = ""; 

    for (let i = 0; i < outline.length; i++) {
        const sectionTitle = outline[i];
        onChunk(`\n\n## ${sectionTitle}\n\n`);
        fullDocument += `## ${sectionTitle}\n\n`;

        const sectionPrompt = `
            ROL: Escritor Acad√©mico Senior.
            TAREA: Escribir la secci√≥n "${sectionTitle}" (Secci√≥n ${i + 1} de ${outline.length}).
            
            ESTRUCTURA DEL PROYECTO:
            ${JSON.stringify(outline)}

            TESIS CENTRAL:
            ${thesis?.title || "N/A"} - ${thesis?.description || ""}
            
            MEMORIA DE TRABAJO (LO YA ESCRITO):
            <previous_content>
            ${fullDocument}
            </previous_content>
            
            INSTRUCCIONES PARA ESTA SECCI√ìN:
            1. Analiza los documentos adjuntos para extraer evidencia NUEVA.
            2. Mant√©n la coherencia estricta con <previous_content>.
        `;

        let sectionDraft = "";
        try {
            const draftResp = await ai.models.generateContent({
                model: model,
                contents: { parts: buildContentParts(sources, sectionPrompt) }
            });
            sectionDraft = draftResp.text || "";

            // Humanizamos
            const humanizedSection = await humanizeContentLogic(ai, sectionDraft, options);
            
            onChunk(humanizedSection);
            fullDocument += humanizedSection + "\n\n";

        } catch (e) {
            onChunk(`_[Error generando secci√≥n ${sectionTitle}]_`);
            console.error(e);
        }
    }

    return fullDocument;
};

// Exportar helpers
export const generateOptimizedPrompt = async (sources: Source[], instruction: string): Promise<string> => {
    const ai = getClient();
    const resp = await ai.models.generateContent({ 
        model: 'gemini-2.0-flash', 
        contents: `Mejora esta instrucci√≥n: "${instruction}"` 
    });
    return resp.text?.trim() || instruction;
};

export const generateStyleGuide = async (samples: string[]): Promise<string> => {
    const ai = getClient();
    const resp = await ai.models.generateContent({ 
        model: 'gemini-2.0-flash', 
        contents: `Analiza este estilo de escritura para crear un "System Prompt" que permita a una IA imitarlo perfectamente. Describe longitud de frases, vocabulario t√≠pico, uso de conectores, tono y peculiaridades gramaticales:\n${samples.join('\n')}` 
    });
    return resp.text || "";
};

export const buildPortablePrompt = (sources: Source[], instruction: string, options: RedactionOptions): string => {
   return `<!-- SYSTEM PROMPT PARA CLAUDE PROJECTS -->
   <role>
   Eres un redactor acad√©mico experto.
   Estilo de Escritura Requerido: ${options.styleGuide || options.tone}
   </role>
   
   <task>
   Escribe un borrador completo sobre: "${instruction}".
   Usa la informaci√≥n de los archivos adjuntos.
   </task>
   `;
};