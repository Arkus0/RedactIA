import { GoogleGenAI, GenerateContentResponse, Type } from "@google/genai";
import { RedactionOptions, Length, Source, Tone, Thesis, StructureType, ModelId } from '../types';

const getClient = () => {
  return new GoogleGenAI({ apiKey: process.env.API_KEY });
};

// Configuraci√≥n agresiva para romper patrones de IA
const getHumanizerConfig = () => {
    return { 
        temperature: 0.95, // Alta temperatura para creatividad y "errores" humanos sutiles
        topP: 0.95,
        maxOutputTokens: 8192 
    };
};

const buildContextBlock = (sources: Source[]) => 
    sources.map(s => `<document name="${s.name}">${s.content.slice(0, 20000)}</document>`).join('\n');

const cleanJsonOutput = (text: string): string => {
  if (!text) return "[]";
  return text.replace(/```json/g, '').replace(/```/g, '').trim();
};

// --- PROMPT PORTABLE (EXPORTACI√ìN) ---
export const buildPortablePrompt = (
  sources: Source[], 
  instruction: string, 
  options: RedactionOptions
): string => {
  const fileManifest = sources.length > 0 
    ? sources.map(s => `    <file_requirement status="PENDING_ATTACHMENT">${s.name}</file_requirement>`).join('\n')
    : "    <file_requirement>El usuario adjuntar√° los documentos relevantes manualmente.</file_requirement>";

  const styleBlock = options.styleGuide 
    ? `  <style_dna_imperative>\n    <description>Mimetiza rigurosamente este estilo:</description>\n    <voice_sample>\n${options.styleGuide}\n    </voice_sample>\n  </style_dna_imperative>`
    : `  <style_configuration>\n    <tone>${options.tone}</tone>\n  </style_configuration>`;

  return `<!-- SYSTEM PROMPT: REDACTA_IA ANTI-DETECTOR v3.0 -->
<system_role>
  Act√∫a como un Escritor Acad√©mico Humano. Tu objetivo es burlar los detectores de IA.
  REGLAS DE ORO:
  1. NO USES LISTAS (Bullet points). Convierte todo en p√°rrafos densos y conectados.
  2. VARIABILIDAD (Burstiness): Mezcla frases muy largas y complejas con frases cortas y contundentes.
  3. VOCABULARIO: Evita palabras de IA como "Cabe destacar", "En conclusi√≥n", "Por otro lado". Usa conectores naturales.
</system_role>

<context_input>
  <manifest_of_expected_files>\n${fileManifest}\n  </manifest_of_expected_files>
</context_input>

<project_configuration>
  <output_format>${options.format}</output_format>
  <target_length>${options.length}</target_length>
  <citation_style>${options.includeCrossReferences ? 'APA 7 (Citas expl√≠citas)' : 'Narrativa'}</citation_style>
</project_configuration>

${styleBlock}

<task_directive>
  ${instruction}
</task_directive>
`.trim();
};

// --- 1. GENERADOR DE TESIS ---
export const generateTheses = async (sources: Source[], instruction: string): Promise<Thesis[]> => {
  const ai = getClient();
  const context = buildContextBlock(sources);
  
  // Usamos Flash para l√≥gica r√°pida
  const prompt = `
    Analiza fuentes e instrucci√≥n.
    <library>${context || "General"}</library>
    <instruction>${instruction}</instruction>
    Genera 3 Tesis (Enfoques) para un trabajo acad√©mico.
    SALIDA: JSON Array v√°lido. [{ "id": "1", "angle": "Cr√≠tico", "title": "...", "description": "..." }]
  `;

  try {
    const resp = await ai.models.generateContent({
      model: 'gemini-2.0-flash', 
      contents: prompt,
      config: { responseMimeType: "application/json" }
    });
    return JSON.parse(cleanJsonOutput(resp.text || "[]"));
  } catch (e) {
    console.error("Error tesis:", e);
    return [];
  }
};

// --- 2. GENERADOR DE √çNDICE (ARQUITECTO) ---
const generateOutline = async (sources: Source[], instruction: string, thesis: Thesis | null, options: RedactionOptions): Promise<string[]> => {
  const ai = getClient();
  const context = buildContextBlock(sources);
  const sectionsCount = options.length === Length.THESIS ? "8-12" : "5-7";
  
  const prompt = `
    Act√∫a como Editor. Crea un √çNDICE para un texto de longitud ${options.length} (${sectionsCount} secciones).
    Tema: ${instruction}. Tesis: ${thesis?.title || "N/A"}.
    Contexto: <library>${context.slice(0, 10000)}</library>
    SALIDA: JSON Array de t√≠tulos. Ej: ["Introducci√≥n", "Desarrollo", "Conclusi√≥n"]
  `;

  try {
    const resp = await ai.models.generateContent({
      model: 'gemini-2.0-flash',
      contents: prompt,
      config: { responseMimeType: "application/json" }
    });
    return JSON.parse(cleanJsonOutput(resp.text || "[]"));
  } catch (e) {
    return ["Introducci√≥n", "Cuerpo Principal", "Conclusi√≥n"];
  }
};

// --- HELPER: HUMANIZADOR (EL SECRETO ANTI-IA) ---
// Esta funci√≥n reescribe el texto l√≥gico para hacerlo "humano" y ca√≥tico
const humanizeContent = async (ai: GoogleGenAI, text: string, options: RedactionOptions, context: string): Promise<string> => {
    const prompt = `
        TU √öNICA MISI√ìN: REESCRIBIR ESTE TEXTO PARA QUE PAREZCA 100% HUMANO.
        
        TEXTO A HUMANIZAR:
        "${text}"

        REGLAS OBLIGATORIAS (ANTI-DETECCI√ìN IA):
        1. üî• **MUERTE A LAS LISTAS**: Si ves una lista con vi√±etas, DESTR√öYELA y convi√©rtela en un p√°rrafo narrativo fluido.
        2. üåä **BURSTINESS (Caos)**: Alterna oraciones muy largas y subordinadas con oraciones de 4 palabras. Rompe el ritmo rob√≥tico.
        3. üö´ **PALABRAS PROHIBIDAS**: No uses "En conclusi√≥n", "En resumen", "Por consiguiente", "Es importante destacar". Usa "Y es que", "Total,", "Al final", "Entonces".
        4. üé≠ **ESTILO**: ${options.styleGuide ? `Imita este estilo: ${options.styleGuide}` : `Tono: ${options.tone}, pero natural.`}
        5. üß¨ **IMPERFECCI√ìN**: S√© directo. No seas pedante.

        ${options.includeCrossReferences ? "Mant√©n las citas bibliogr√°ficas si existen." : ""}
    `;

    // Usamos gemini-2.0-flash con ALTA temperatura porque es menos r√≠gido que los modelos Pro
    const resp = await ai.models.generateContent({
        model: 'gemini-2.0-flash',
        contents: prompt,
        config: getHumanizerConfig()
    });

    return resp.text || text;
};

// --- 3. MOTOR DE REDACCI√ìN ---
export const generateRedaction = async (
  sources: Source[],
  instruction: string, 
  options: RedactionOptions,
  selectedThesis: Thesis | null,
  onChunk: (c: string) => void,
  onReset: () => void
): Promise<string> => {
  const ai = getClient();
  const isModular = options.length === Length.LONG || options.length === Length.THESIS;
  
  // Usamos el modelo seleccionado para la l√≥gica, pero Flash para humanizar
  const logicModel = options.model || ModelId.GEMINI_3_FLASH;

  if (isModular) {
    return generateModularRedaction(ai, logicModel, sources, instruction, options, selectedThesis, onChunk, onReset);
  } else {
    return generateSinglePassRedaction(ai, logicModel, sources, instruction, options, selectedThesis, onChunk, onReset);
  }
};

// --- ESTRATEGIA CORTA (SINGLE PASS + HUMANIZER) ---
const generateSinglePassRedaction = async (ai: any, model: string, sources: Source[], instruction: string, options: RedactionOptions, thesis: any, onChunk: any, onReset: any) => {
    const context = buildContextBlock(sources);
    
    // FASE 1: BORRADOR L√ìGICO (Streaming)
    const draftPrompt = `
        Escribe un borrador acad√©mico s√≥lido.
        Tema: ${instruction}. Tesis: ${thesis?.title || "N/A"}.
        Fuentes: ${context}.
        Longitud: ${options.length}.
        ESTRUCTURA: Introducci√≥n, Desarrollo, Conclusi√≥n.
        NOTA: Prioriza la estructura y los argumentos.
    `;

    let draft = "";
    // Si el usuario eligi√≥ un modelo Thinking (Gemini 3 Pro), aqu√≠ brilla
    const config: any = { maxOutputTokens: 8192 };
    if (model.includes('gemini-3-pro')) config.thinkingConfig = { thinkingBudget: 2048 };

    const draftResp = await ai.models.generateContentStream({
        model: model,
        contents: draftPrompt,
        config: config
    });

    for await (const chunk of draftResp) {
        const t = (chunk as GenerateContentResponse).text || '';
        draft += t;
        onChunk(t); // Mostramos el borrador mientras se genera
    }

    // FASE 2: HUMANIZACI√ìN (Si no es borrador r√°pido)
    // Siempre aplicamos una capa de pulido para evitar el "Robot Voice", a menos que sea muy corto
    if (options.length !== Length.SHORT) {
        onReset();
        onChunk(draft); // Restauramos visualmente
        onChunk("\n\n_üß¨ Humanizando texto para evitar detecci√≥n IA..._");
        
        const humanized = await humanizeContent(ai, draft, options, context);
        onReset();
        onChunk(humanized);
        return humanized;
    }

    return draft;
};

// --- ESTRATEGIA LARGA (MODULAR + HUMANIZER PER SECTION) ---
const generateModularRedaction = async (ai: any, model: string, sources: Source[], instruction: string, options: RedactionOptions, thesis: any, onChunk: any, onReset: any) => {
    const context = buildContextBlock(sources);
    
    onChunk(`_üèóÔ∏è Arquitecto dise√±ando estructura..._\n\n`);
    const outline = await generateOutline(sources, instruction, thesis, options);
    onReset();

    let fullDocument = "";
    let previousContext = "Inicio.";

    for (let i = 0; i < outline.length; i++) {
        const sectionTitle = outline[i];
        onChunk(`\n\n## ${sectionTitle}\n\n`);
        fullDocument += `## ${sectionTitle}\n\n`;

        // 1. Generar Borrador de Secci√≥n
        const sectionPrompt = `
            Escribe la secci√≥n "${sectionTitle}" de un trabajo sobre ${instruction}.
            Contexto previo: ${previousContext.slice(-2000)}.
            Fuentes: ${context}.
            Solo contenido, sin pre√°mbulos.
        `;

        let sectionDraft = "";
        try {
            // Usamos un modelo r√°pido para el borrador de secci√≥n para no hacer esperar tanto
            const draftResp = await ai.models.generateContent({
                model: 'gemini-2.0-flash', 
                contents: sectionPrompt
            });
            sectionDraft = draftResp.text || "";
            
            // 2. Humanizar Secci√≥n INMEDIATAMENTE
            // Esto asegura que la siguiente secci√≥n use contexto "humano" y no "robot"
            const humanizedSection = await humanizeContent(ai, sectionDraft, options, "");
            
            onChunk(humanizedSection); // Mostramos solo la versi√≥n final
            fullDocument += humanizedSection + "\n\n";
            previousContext += `\nResumen ${sectionTitle}: ${humanizedSection.slice(0, 300)}...`;

        } catch (e) {
            onChunk(`_[Error en secci√≥n ${sectionTitle}]_`);
        }
    }

    return fullDocument;
};

export const generateOptimizedPrompt = async (sources: Source[], instruction: string): Promise<string> => {
    const ai = getClient();
    const resp = await ai.models.generateContent({ 
        model: 'gemini-2.0-flash', 
        contents: `Mejora esta instrucci√≥n para una IA: "${instruction}". Hazla detallada.` 
    });
    return resp.text?.trim() || instruction;
};

export const generateStyleGuide = async (samples: string[]): Promise<string> => {
    const ai = getClient();
    const resp = await ai.models.generateContent({ 
        model: 'gemini-2.0-flash', 
        contents: `Analiza este estilo de escritura y crea un prompt para imitarlo (vocabulario, longitud de frases, tono):\n${samples.join('\n')}` 
    });
    return resp.text || "";
};